{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Quora question answer dataset to build a model that could identify similar questions.\n",
    "This is a useful task because you don't want to have several versions of the same question posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.supervised import training\n",
    "from trax.fastmath import numpy as fastnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question pairs:  404351\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>191</td>\n",
       "      <td>192</td>\n",
       "      <td>How does 3D printing work?</td>\n",
       "      <td>How do 3D printing work?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>193</td>\n",
       "      <td>194</td>\n",
       "      <td>What was it like to attend Caltech with Jeremy...</td>\n",
       "      <td>Who are some notable folks who attended Caltech?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>195</td>\n",
       "      <td>196</td>\n",
       "      <td>Why did harry become a horcrux?</td>\n",
       "      <td>What is a Horcrux?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>What are the best associate product manager (A...</td>\n",
       "      <td>What are the general requirement to become a P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "      <td>Why is the number for Skype at 1-855-425-3768 ...</td>\n",
       "      <td>How could I get Skype to work on an android 4....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  qid1  qid2                                          question1  \\\n",
       "0    0     1     2  What is the step by step guide to invest in sh...   \n",
       "1    1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2    2     5     6  How can I increase the speed of my internet co...   \n",
       "3    3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4    4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "..  ..   ...   ...                                                ...   \n",
       "95  95   191   192                         How does 3D printing work?   \n",
       "96  96   193   194  What was it like to attend Caltech with Jeremy...   \n",
       "97  97   195   196                    Why did harry become a horcrux?   \n",
       "98  98   197   198  What are the best associate product manager (A...   \n",
       "99  99   199   200  Why is the number for Skype at 1-855-425-3768 ...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  \n",
       "..                                                ...           ...  \n",
       "95                           How do 3D printing work?             1  \n",
       "96   Who are some notable folks who attended Caltech?             0  \n",
       "97                                 What is a Horcrux?             0  \n",
       "98  What are the general requirement to become a P...             0  \n",
       "99  How could I get Skype to work on an android 4....             0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"questions.csv\")\n",
    "N=len(data)\n",
    "print('Number of question pairs: ', N)\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 300000 Test set: 10240\n"
     ]
    }
   ],
   "source": [
    "#train-test split\n",
    "N_train = 300000\n",
    "N_test  = 10*1024\n",
    "data_train = data[:N_train]\n",
    "data_test  = data[N_train:N_train+N_test]\n",
    "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
    "del(data) # remove to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two batches as input for the Siamese network and assume that question $q1_i$ (question $i$ in the first batch) is a duplicate of $q2_i$ (question $i$ in the second batch), but all other questions in the second batch are not duplicates of $q1_i$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate questions:  111486\n",
      "indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
     ]
    }
   ],
   "source": [
    "td_index = (data_train['is_duplicate'] == 1).to_numpy()\n",
    "td_index = [i for i, x in enumerate(td_index) if x] \n",
    "print('number of duplicate questions: ', len(td_index))\n",
    "print('indexes of first ten duplicate questions:', td_index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the duplicated questions for training the model. <br> Two batches are produced such that $([q1_1, q1_2, q1_3, ...]$, $[q2_1, q2_2,q2_3, ...])$  where $q1_i$ and $q2_k$ are duplicate if and only if $i = k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_train_words = np.array(data_train['question1'][td_index])\n",
    "Q2_train_words = np.array(data_train['question2'][td_index])\n",
    "\n",
    "Q1_test_words = np.array(data_test['question1'])\n",
    "Q2_test_words = np.array(data_test['question2'])\n",
    "y_test  = np.array(data_test['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUESTIONS:\n",
      "\n",
      "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
      "\n",
      "Question 1:  What would a Trump presidency mean for current international masterâ€™s students on an F1 visa?\n",
      "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
      "\n",
      "TESTING QUESTIONS:\n",
      "\n",
      "Question 1:  How do I prepare for interviews for cse?\n",
      "Question 2:  What is the best way to prepare for cse? \n",
      "\n",
      "is_duplicate = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_train_words[0])\n",
    "print('Question 2: ', Q2_train_words[0], '\\n')\n",
    "print('Question 1: ', Q1_train_words[5])\n",
    "print('Question 2: ', Q2_train_words[5], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_test_words[0])\n",
    "print('Question 2: ', Q2_test_words[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create arrays\n",
    "Q1_train = np.empty_like(Q1_train_words)\n",
    "Q2_train = np.empty_like(Q2_train_words)\n",
    "\n",
    "Q1_test = np.empty_like(Q1_test_words)\n",
    "Q2_test = np.empty_like(Q2_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is:  36306\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "vocab = defaultdict(lambda: 0)\n",
    "vocab['<PAD>'] = 1\n",
    "\n",
    "for idx in range(len(Q1_train_words)):\n",
    "    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n",
    "    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n",
    "    q = Q1_train[idx] + Q2_train[idx]\n",
    "    for word in q:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "print('The length of the vocabulary is: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(Q1_test_words)): \n",
    "    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])\n",
    "    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has reduced to:  111486\n",
      "Test set length:  10240\n"
     ]
    }
   ],
   "source": [
    "print('Train set has reduced to: ', len(Q1_train) ) \n",
    "print('Test set length: ', len(Q1_test) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Converting a question to a tensor\n",
    "Convert every question to a tensor, or an array of numbers, using the vocabulary built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting questions to array of integers\n",
    "for i in range(len(Q1_train)):\n",
    "    Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n",
    "    Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n",
    "\n",
    "        \n",
    "for i in range(len(Q1_test)):\n",
    "    Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n",
    "    Q2_test[i] = [vocab[word] for word in Q2_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first question in the train set:\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
      "\n",
      "encoded version:\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] \n",
      "\n",
      "first question in the test set:\n",
      "\n",
      "How do I prepare for interviews for cse? \n",
      "\n",
      "encoded version:\n",
      "[32, 38, 4, 107, 65, 1015, 65, 11514, 21]\n"
     ]
    }
   ],
   "source": [
    "print('first question in the train set:\\n')\n",
    "print(Q1_train_words[0], '\\n') \n",
    "print('encoded version:')\n",
    "print(Q1_train[0],'\\n')\n",
    "\n",
    "print('first question in the test set:\\n')\n",
    "print(Q1_test_words[0], '\\n')\n",
    "print('encoded version:')\n",
    "print(Q1_test[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train set into a training/validation set to train and evaluate your Siamese model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "The length of the training set is:   89188\n",
      "The length of the validation set is:  22298\n"
     ]
    }
   ],
   "source": [
    "cut_off = int(len(Q1_train)*.8)\n",
    "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
    "val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]\n",
    "print('Number of duplicate questions: ', len(Q1_train))\n",
    "print(\"The length of the training set is:  \", len(train_Q1))\n",
    "print(\"The length of the validation set is: \", len(val_Q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a data generator that takes in $Q1$ and $Q2$ and returns a batch of size batch_size  in the following format $([q1_1, q1_2, q1_3, ...]$, $[q2_1, q2_2,q2_3, ...])$. The tuple consists of two arrays and each array has batch_size questions. Again, $q1_i$ and $q2_i$ are duplicates, but they are not duplicates with any other elements in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        Q1 (list): List of transformed (to tensor) questions.\n",
    "        Q2 (list): List of transformed (to tensor) questions.\n",
    "        batch_size (int): Number of elements per batch.\n",
    "        pad (int, optional): Pad character from the vocab. Defaults to 1.\n",
    "        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.\n",
    "    Yields:\n",
    "        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    input1 = []\n",
    "    input2 = []\n",
    "    idx = 0\n",
    "    len_q = len(Q1)\n",
    "    question_indexes = [*range(len_q)]\n",
    "    \n",
    "    if shuffle:\n",
    "        rnd.shuffle(question_indexes)\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        if idx >= len_q:\n",
    "            # if idx is greater than or equal to len_q, set idx accordingly \n",
    "            idx = 0\n",
    "            # shuffle to get random batches if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(question_indexes)\n",
    "        \n",
    "        # get questions at the `question_indexes[idx]` position in Q1 and Q2\n",
    "        q1 = Q1[question_indexes[idx]]\n",
    "        q2 = Q2[question_indexes[idx]]\n",
    "        \n",
    "        # increment idx by 1\n",
    "        idx += 1\n",
    "        # append q1\n",
    "        input1.append(q1)\n",
    "        # append q2\n",
    "        input2.append(q2)\n",
    "        if len(input1) == batch_size:\n",
    "            # determine max_len as the longest question in input1 & input 2\n",
    "            max_len = max(max(map(lambda z: len(z), input1)),max(map(lambda z: len(z), input2)))\n",
    "            max_len = 2**int(np.ceil(np.log2(max_len)))\n",
    "            b1 = []\n",
    "            b2 = []\n",
    "            for q1, q2 in zip(input1, input2):\n",
    "                # add [pad] to q1 until it reaches max_len\n",
    "                q1 = q1 + [pad] * (max_len - len(q1))\n",
    "                # add [pad] to q2 until it reaches max_len\n",
    "                q2 = q2 + [pad] * (max_len - len(q2))\n",
    "                b1.append(q1)\n",
    "                b2.append(q2)\n",
    "            yield np.array(b1), np.array(b2)\n",
    "            # reset the batches\n",
    "            input1, input2 = [], [] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.\n",
    "\n",
    "<img src = \"siamese.png\" style=\"width:600px;height:300px;\"/>\n",
    "\n",
    "Get the question embedding, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally use a triplet loss to get the corresponding cosine similarity for each pair of questions. The triplet loss makes use of a baseline input that is compared to a positive input and a negative input. The distance from the baseline input to the positive input is minimized, and the distance from the baseline input to the negative input is maximized. \n",
    "\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right)$$\n",
    "\n",
    "$A$ is the anchor input, for example $q1_1$, $P$ the duplicate input, for example, $q2_1$, and $N$ the negative input (the non duplicate question), for example $q2_2$.<br>\n",
    "$\\alpha$ is a margin; by how much you want to push the duplicates from the non duplicates. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Siamese(vocab_size=len(vocab), d_model=128, mode='train'):\n",
    "    \"\"\"Returns a Siamese model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Length of the vocabulary. Defaults to len(vocab).\n",
    "        d_model (int, optional): Depth of the model. Defaults to 128.\n",
    "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to 'train'.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Parallel: A Siamese model. \n",
    "    \"\"\"\n",
    "\n",
    "    def normalize(x):  # normalizes the vectors to have L2 norm 1\n",
    "        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n",
    "    \n",
    "    \n",
    "    q_processor = tl.Serial(  \n",
    "        tl.Embedding(vocab_size, d_model), # Embedding layer\n",
    "        tl.LSTM(d_model), # LSTM layer\n",
    "        tl.Mean(axis = 1), # Mean over columns\n",
    "        tl.Fn('Normalize', lambda x: normalize(x))  \n",
    "    )  \n",
    "    # Returns one vector of shape [batch_size, d_model].\n",
    "    \n",
    "\n",
    "    model = tl.Parallel(q_processor, q_processor)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel_in2_out2[\n",
      "  Serial[\n",
      "    Embedding_41747_128\n",
      "    LSTM_128\n",
      "    Mean\n",
      "    Normalize\n",
      "  ]\n",
      "  Serial[\n",
      "    Embedding_41747_128\n",
      "    LSTM_128\n",
      "    Mean\n",
      "    Normalize\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = Siamese()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the *closest negative*.\n",
    " \n",
    "\\begin{align}\n",
    " \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n",
    " \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n",
    "\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    \"\"\"Custom Loss function.\n",
    "\n",
    "    Args:\n",
    "        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.\n",
    "        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.\n",
    "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Triplet Loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    scores = fastnp.dot(v1, v2.T)\n",
    "    \n",
    "    # calculate new batch size\n",
    "    batch_size = len(scores)\n",
    "    \n",
    "    # use fastnp to grab all postive `diagonal` entries in `scores`\n",
    "    positive = fastnp.diagonal(scores)\n",
    "    \n",
    "    # multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`\n",
    "    negative_without_positive = scores - 2 * fastnp.eye(batch_size)\n",
    "    \n",
    "    # take the row by row `max` of `negative_without_positive`. \n",
    "    closest_negative = negative_without_positive.max(axis = 1)\n",
    "    \n",
    "    # subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`\n",
    "    negative_zero_on_duplicate = scores * (1 - fastnp.eye(batch_size))\n",
    "    \n",
    "    # use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` \n",
    "    mean_negative = fastnp.sum(negative_zero_on_duplicate, axis = 1)/ (batch_size - 1)\n",
    "    \n",
    "    # compute `fastnp.maximum` among 0.0 and `A`\n",
    "    # A = subtract `positive` from `margin` and add `closest_negative` \n",
    "    triplet_loss1 = fastnp.maximum(closest_negative - positive + margin , 0)\n",
    "    \n",
    "    # compute `fastnp.maximum` among 0.0 and `B`\n",
    "    # B = subtract `positive` from `margin` and add `mean_negative`\n",
    "    triplet_loss2 = fastnp.maximum(mean_negative - positive + margin , 0)\n",
    "    \n",
    "    # add the two losses together and take the `fastnp.mean` of it\n",
    "    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def TripletLoss(margin=0.25):\n",
    "    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
    "    return tl.Fn('TripletLoss', triplet_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Q1.shape  (89188,)\n",
      "val_Q1.shape    (22298,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab['<PAD>'])\n",
    "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab['<PAD>'])\n",
    "print('train_Q1.shape ', train_Q1.shape)\n",
    "print('val_Q1.shape   ', val_Q1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
    "\n",
    "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir='model/'):\n",
    "    \"\"\"Training the Siamese Model\n",
    "\n",
    "    Args:\n",
    "        Siamese (function): Function that returns the Siamese model.\n",
    "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
    "        lr_schedule (function): Trax multifactor schedule function.\n",
    "        train_generator (generator, optional): Training generator. Defaults to train_generator.\n",
    "        val_generator (generator, optional): Validation generator. Defaults to val_generator.\n",
    "        output_dir (str, optional): Path to save model to. Defaults to 'model/'.\n",
    "\n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop for the model.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.expanduser(output_dir)\n",
    "\n",
    "    \n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator,       \n",
    "        loss_layer=TripletLoss(),         \n",
    "        optimizer=trax.optimizers.Adam(0.01),       \n",
    "        lr_schedule=lr_schedule, \n",
    "    )\n",
    "    \n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator,       \n",
    "        metrics=[TripletLoss()],          \n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    training_loop = training.Loop(Siamese(),\n",
    "                                  train_task,\n",
    "                                  eval_tasks = [eval_task],\n",
    "                                  output_dir=output_dir)\n",
    "\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 5\n",
    "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
    "training_loop.run(train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanna/anaconda3/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "model = Siamese()\n",
    "model.init_from_file('model.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n",
    "    \"\"\"Function to test the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        test_Q1 (numpy.ndarray): Array of Q1 questions.\n",
    "        test_Q2 (numpy.ndarray): Array of Q2 questions.\n",
    "        y (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (trax.layers.combinators.Parallel): The Siamese model.\n",
    "        vocab (collections.defaultdict): The vocabulary used.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model.\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    \n",
    "    for i in range(0, len(test_Q1), batch_size):\n",
    "        q1, q2 = next(data_generator(test_Q1[i: i+batch_size], test_Q2[i: i+batch_size], batch_size, pad=vocab['<PAD>'], shuffle=False))\n",
    "        y_test = y[i : i+batch_size]\n",
    "        v1, v2 = model((q1, q2))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            d = np.dot(v1[j], v2[j].T)\n",
    "            res = int(d > threshold)\n",
    "            \n",
    "            # increment accurancy if y_test is equal `res`\n",
    "            accuracy += (y_test[j] == res)\n",
    "   \n",
    "    accuracy = accuracy / len(test_Q1)\n",
    "    \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7044921875\n"
     ]
    }
   ],
   "source": [
    "accuracy = classify(Q1_test,Q2_test, y_test, 0.7, model, vocab, batch_size = 512) \n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
    "    \"\"\"Function for predicting if two questions are duplicates.\n",
    "\n",
    "    Args:\n",
    "        question1 (str): First question.\n",
    "        question2 (str): Second question.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (trax.layers.combinators.Parallel): The Siamese model.\n",
    "        vocab (collections.defaultdict): The vocabulary used.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the questions are duplicates, False otherwise.\n",
    "    \"\"\"\n",
    "    q1 = nltk.word_tokenize(question1)  # tokenize\n",
    "    q2 = nltk.word_tokenize(question2)  # tokenize\n",
    "    Q1, Q2 = [], []\n",
    "    for word in q1:  \n",
    "        Q1 += [vocab[word]]\n",
    "    for word in q2: \n",
    "        Q2 += [vocab[word]]\n",
    "    \n",
    "    \n",
    "    Q1, Q2 = next(data_generator([Q1], [Q2], batch_size = 1, pad=vocab['<PAD>']))\n",
    "\n",
    "    \n",
    "    v1, v2 = model((Q1, Q2))\n",
    "    d = np.dot(v1, v2.T)\n",
    "    \n",
    "    res = d > threshold\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Q1  = \", Q1, \"\\nQ2  = \", Q2)\n",
    "        print(\"d   = \", d)\n",
    "        print(\"res = \", res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "from tkinter import ttk\n",
    "\n",
    "win = tk.Tk()\n",
    "\n",
    "win.title('Question Similarity Detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarity():\n",
    "    question1 = q1.get()\n",
    "    question2 = q2.get()\n",
    "    \n",
    "    answer = predict(question1 , question2, 0.7, model, vocab, verbose=False)\n",
    "    tk.Label(win, text = answer[0][0]).grid(row = 2, column = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.Label(win, text=\"Question 1\").grid(row=0)\n",
    "tk.Label(win, text=\"Question 2\").grid(row=1)\n",
    "\n",
    "q1 = tk.Entry(win)\n",
    "q1.grid(row=0, column=1)\n",
    "\n",
    "q2 = tk.Entry(win)\n",
    "q2.grid(row=1, column=1)\n",
    "\n",
    "\n",
    "tk.Button(win,text='Detect Similarity', command=findSimilarity).grid(row=2)\n",
    "\n",
    "\n",
    "    \n",
    "win.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
